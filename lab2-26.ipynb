{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f20911",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING\n",
    "\n",
    "**_Experimenting with Transfer Learning for a classification task._**\n",
    "\n",
    "In transfer learning, features learned on one problem are taken and leveraged them on a new and similar problem. It is usually done for tasks where dataset has too little to train a full-scale model from scratch. \n",
    "\n",
    "**The Experiment:**\n",
    "\n",
    "- Loads _MobileNetV2_ model - pretrained on _imagenet_ dataset, as a base model taking all layers except the top one that is used for classification specific to ImageNet task.\n",
    "\n",
    "- Freezes all the layers in the base model to avoid destroying already learned parameters during training related to new task to classify cats and dogs.\n",
    "\n",
    "- Adds few new, trainable layers such as pooling, dropout and dense layer on top of the frozen layers for them to learn based on the new dataset.\n",
    "\n",
    "- Prepares the new dataset in a form that is acceptable by the base model.\n",
    "\n",
    "- Trains the head (newly added layers) on the new data over few epochs to get them trained.\n",
    "\n",
    "- Unfreezes the all the layers in the base model and fine-tunes the entire model by re-training it on the new data with a very low learning rate to achieve meaningful improvements by incrementally adapting the pretrained features to the new data.\n",
    "\n",
    "- Evaluates the model performance on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24d6d5",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ff9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:03:49.637423: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-22 03:03:50.163596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-22 03:03:51.946788: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f8e69",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520006be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Counts:\n",
      "Training: 9305, Validation: 2326, Test: 2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:04:03.942728: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Fetches the \"cats vs. dogs\" dataset using TFDS\n",
    "# Only 40% of the data is used to show the effectiveness of transfer learning\n",
    "train_set, val_set, test_set = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],  # 40% for training, 10% for validation and 10% for test\n",
    "    as_supervised=True,                                         # Includes labels\n",
    ")\n",
    "\n",
    "print(\"Sample Counts:\")\n",
    "print(f\"Training: {train_set.cardinality()}, Validation: {val_set.cardinality()}, Test: {test_set.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a069d",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fb7af",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b00cb8",
   "metadata": {},
   "source": [
    "As the images size varies and each pixel consists of 3 integer values between 0 and 255 (RGB level values), all the images get resized to same size of 160x160 and pixel values get normalized between -1 and 1 according to the requirement of the pretrained model. Additionally, random data augmentation is also applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571f482",
   "metadata": {},
   "source": [
    "**Resizing images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39cb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer =tf.keras.layers.Resizing(160 , 160 ) # Code here to use `tf.keras.layers.Resizing` as function to resize image to 160x160\n",
    "\n",
    "# Maps each sample from the each of the datasets and performs resizing\n",
    "# Perform the image resizing by calling `map` lambda function of each dataset \n",
    "# passing resizer (with labels)\n",
    "\n",
    "train_set = train_set.map(lambda x, y :(resizer(x), y))# Code here\n",
    "val_set = val_set.map(lambda x, y :(resizer(x), y))# Code here\n",
    "test_set =test_set.map(lambda x, y :(resizer(x), y))# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6492c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([160, 160, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirms the resizing just by taking one sample with an index\n",
    "next(iter(train_set))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed7334",
   "metadata": {},
   "source": [
    "**Augmenting Data**\n",
    "\n",
    "Applying data augmentation to generate samples artificially by applying random and realistic transformations to the training images, such as random horizontal flipping or small random rotations helping the model to get exposed to different aspects of the training data while slowing down overfitting. It makes sense when dataset is not large or not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a15afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the sequence of operations (once again taking layers as utulity functions)\n",
    "augmentation_layers = [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "def data_augmentation(x):\n",
    "    \"\"\"\n",
    "    Augments the image by flipping and rotating it\n",
    "    \"\"\"\n",
    "    for layer in augmentation_layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "# Use `data_augmentation` function passing it to train set's `map` lambda function to augment the images\n",
    "train_set =train_set.map(lambda x, y :(data_augmentation(x), y)) # Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865e92a",
   "metadata": {},
   "source": [
    "**Batching Data and Optimizing Loading Speed using Prefetching and Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756f885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = train_set.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n",
    "val_set = val_set.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n",
    "test_set = test_set.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfe26e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe53d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate `tf.keras.applications.MobileNetV2` as base model specifying \n",
    "# (160, 160, 3) as `input_shape`, 'False` to `include_top` to not to include top layer and\n",
    "# `imagenet` as `weights` to initialize the model with specific weights\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(160, 160, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "    \n",
    "   \n",
    "\n",
    "base_model.trainable=False# Code to freeze all layers in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1039443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target model out of base model\n",
    "\n",
    "inputs =tf.keras.layers.Input(shape=(160, 160 , 3))    # Code to initialize the model input with expectation of `shape` (160, 160, 3)\n",
    "\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)      # Code to pass `inputs` to routine `tf.keras.applications.mobilenet_v2.preprocess_input` to scale input pixels between -1 and 1\n",
    "\n",
    "\n",
    "x = base_model(x, training=False)      # Code to pass `x` to base model `base_model`. Also set `training` parameter to `False` to keep model in inference mode\n",
    "\n",
    "x =tf.keras.layers.GlobalAveragePooling2D()(x)     # Code to call `tf.keras.layers.GlobalAveragePooling2D()` passing `x` to convert base model's multi-dimensional output (`base_model.output_shape[1:]`) to vectors\n",
    "\n",
    "x =tf.keras.layers.Dropout(0.2)(x)    # Code to call `tf.keras.layers.Dropout` passing argument 0.2 to initialize and then pass `x` to initialized class to regularize the network with 20% dropout\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(x)     # Call to `tf.keras.layers.Dense` with initialize a dense layer with 1 unit and then pass `x` to the layer\n",
    "\n",
    "model =tf.keras.Model(inputs,outputs)     # Create target model calling `tf.keras.Model` passing both `inputs` and `outputs` as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f78b1440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Trai… </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ true_divide_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ subtract_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ mobilenetv2_1.00_160        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │   <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">N</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ global_average_pooling2d_2  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "└─────────────────────────────┴───────────────────────┴────────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTrai…\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ true_divide_2 (\u001b[38;5;33mTrueDivide\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ subtract_2 (\u001b[38;5;33mSubtract\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ mobilenetv2_1.00_160        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)    │  \u001b[38;5;34m2,257,984\u001b[0m │   \u001b[1;91mN\u001b[0m   │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ global_average_pooling2d_2  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │      \u001b[38;5;34m1,281\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "└─────────────────────────────┴───────────────────────┴────────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary(show_trainable=True)      # Shows the model summary with both trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb6318",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d384f2",
   "metadata": {},
   "source": [
    "### Training Head of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49eec0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 282ms/step - binary_accuracy: 0.9413 - loss: 0.1381 - val_binary_accuracy: 0.9742 - val_loss: 0.0628\n",
      "Epoch 2/2\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 239ms/step - binary_accuracy: 0.9684 - loss: 0.0825 - val_binary_accuracy: 0.9785 - val_loss: 0.0551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7265c4552510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model by calling model's `compile` method passing `adam` as `optimizer`, `tf.keras.losses.BinaryCrossentropy(from_logits=True)` as `loss`,\n",
    "# [tf.keras.metrics.BinaryAccuracy()] as metrics\n",
    "# Code here\n",
    "\n",
    "# Call model's `fit` method to fit the model on train set with 2 epochs. \n",
    "# Also pass validation set to parameter `validation_data` to measures the learning during training\n",
    "# Code here\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "model.fit(train_set, epochs=2 , validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f728bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 217ms/step - binary_accuracy: 0.9785 - loss: 0.0606\n",
      "Test set performance after head training:\n",
      "       Loss: 0.06, Accuracy: 97.85%\n"
     ]
    }
   ],
   "source": [
    "# Evaluates the model performance post head training\n",
    "\n",
    "post_head_train_perf = model.evaluate(test_set)\n",
    "\n",
    "print(f\"Test set performance after head training:\\n \\\n",
    "      Loss: {post_head_train_perf[0]:.2f}, Accuracy: {post_head_train_perf[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eae191",
   "metadata": {},
   "source": [
    "### Fine-tuning the Model\n",
    "Unfreezing the base model and training the entire model end-to-end with a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66123636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set (all layers of) base model as not-trainable\n",
    "base_model.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d007ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Trai… </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ true_divide_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ subtract_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ mobilenetv2_1.00_160        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ global_average_pooling2d_2  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "└─────────────────────────────┴───────────────────────┴────────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTrai…\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ true_divide_2 (\u001b[38;5;33mTrueDivide\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ subtract_2 (\u001b[38;5;33mSubtract\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ mobilenetv2_1.00_160        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)    │  \u001b[38;5;34m2,257,984\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ global_average_pooling2d_2  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │      \u001b[38;5;34m1,281\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "└─────────────────────────────┴───────────────────────┴────────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,829</span> (8.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,261,829\u001b[0m (8.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,225,153</span> (8.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,225,153\u001b[0m (8.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,112</span> (133.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34,112\u001b[0m (133.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,564</span> (10.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,564\u001b[0m (10.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5e90a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 1s/step - binary_accuracy: 0.9171 - loss: 0.2254 - val_binary_accuracy: 0.9716 - val_loss: 0.0984\n",
      "Epoch 2/5\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 1s/step - binary_accuracy: 0.9622 - loss: 0.0945 - val_binary_accuracy: 0.9759 - val_loss: 0.0733\n",
      "Epoch 3/5\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 1s/step - binary_accuracy: 0.9788 - loss: 0.0585 - val_binary_accuracy: 0.9768 - val_loss: 0.0655\n",
      "Epoch 4/5\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 1s/step - binary_accuracy: 0.9913 - loss: 0.0353 - val_binary_accuracy: 0.9776 - val_loss: 0.0604\n",
      "Epoch 5/5\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 1s/step - binary_accuracy: 0.9959 - loss: 0.0216 - val_binary_accuracy: 0.9794 - val_loss: 0.0599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7265c42f0690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model by calling model's `compile` method passing `tf.keras.optimizers.Adam(1e-5)` as `optimizer`, \n",
    "# `tf.keras.losses.BinaryCrossentropy(from_logits=True)` as `loss`,\n",
    "# [tf.keras.metrics.BinaryAccuracy()] as metrics\n",
    "# Code here\n",
    "\n",
    "# Call model's `fit` method to fit the model on train set with 5 epochs. \n",
    "# Also pass validation set to parameter `validation_data` to measures the learning during training\n",
    "# Code here\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics= [tf.keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(train_set, epochs = 5 , validation_data= val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ab48c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 192ms/step - binary_accuracy: 0.9815 - loss: 0.0617\n",
      "Test set performance after full model training:\n",
      "       Loss: 0.06, Accuracy: 98.15%\n"
     ]
    }
   ],
   "source": [
    "# Evaluates the model performance post fine-tuning the full model\n",
    "\n",
    "post_full_model_train_perf = model.evaluate(test_set)\n",
    "\n",
    "print(f\"Test set performance after full model training:\\n \\\n",
    "      Loss: {post_full_model_train_perf[0]:.2f}, Accuracy: {post_full_model_train_perf[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e71f1",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Did the model learn over less data? Explain in detail.\n",
    "\n",
    "- Why is the reason data augmentation was used for?\n",
    "\n",
    "- List general workflow of transfer learning followed in this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
